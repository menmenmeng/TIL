{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __sklearn__\n",
    "## 1. 사이킷런 소개, 특징\n",
    "## 2. 붓꽃 품종 예측\n",
    "## 3. 사이킷런 기반 프레임워크\n",
    "## 4. model_selection\n",
    "## 5. preprocessing\n",
    "## 6. 타이타닉 생존자 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 붓꽃 품종 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 : 0.9333\n"
     ]
    }
   ],
   "source": [
    "# 예제 데이터 로드\n",
    "iris = load_iris()\n",
    "\n",
    "# 피쳐, 레이블 데이터 저장\n",
    "iris_data = iris.data # 피쳐 데이터(nparray)\n",
    "iris_label = iris.target # 레이블 데이터(nparray)\n",
    "\n",
    "# DataFrame 만들기\n",
    "iris_df = pd.DataFrame(iris_data, columns = iris.feature_names)\n",
    "iris_df['label'] = iris_label\n",
    "iris_df.head(3)\n",
    "\n",
    "# train, test dataset 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2,\n",
    "                                                    random_state = 11)\n",
    "\n",
    "# decision tree classifier 객체 생성하고 모델 적합, 예측\n",
    "dt_clf = DecisionTreeClassifier(random_state=11)\n",
    "dt_clf.fit(X_train, y_train) \n",
    "pred = dt_clf.predict(X_test)\n",
    "\n",
    "# 예측 성능 평가 : 정확도 측정하기.\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('예측 정확도 : {0:.4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 적합, 예측 순서\n",
    "1. 피쳐 데이터, 레이블 데이터 저장\n",
    "2. DataFrame 저장\n",
    "3. train, test set 분리\n",
    "4. 모델 적합\n",
    "5. 예측 및 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 사이킷런 기반 프레임워크\n",
    "\n",
    "- Estimator(분류, 회귀 모델들) - fit(), predict()\n",
    " - fit()을 통해 모델 적합, predict()를 통해 원하는 데이터 예측\n",
    " \n",
    "- 비지도 학습 모델 - fit(), transform(), fit_transform()\n",
    " - fit()을 통해 데이터 변환방식 만들고, transform()을 통해 다른 데이터를 같은 방식으로 변환\n",
    " - fit_transform()을 test data에 사용하지 말 것!!\n",
    " \n",
    "- 그 외, Estimator를 인자로 받는 cross_val_score(), GridSearchCV()에 대해서."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn 주요 모듈\n",
    "#### 예제 데이터\n",
    "- sklearn.datasets # 예시 파일들\n",
    "\n",
    "#### 피처 처리(전처리)\n",
    "- sklearn.preprocessing # 문자열 숫자형으로, 정규화, 스케일링 등 전처리 과정\n",
    "- sklearn.feature_selection # 변수 선택(더 중요한 변수 선택하는 기법).\n",
    "- sklearn.feature_extraction # 변수 추출.. 텍스트 데이터, 이미지 데이터 어쩌구 아직은 모름\n",
    "\n",
    "#### 피처 처리(차원 축소)\n",
    "- sklearn.decomposition # 차원 축소 관련 (PCA, NMF, Truncated SVD)\n",
    "\n",
    "#### 데이터 분리, 검증, 파라미터 튜닝\n",
    "- sklearn.model_selection # 학습용/테스트용 데이터 분리, 그리드 서치로 최적 하이퍼 파라미터 추출\n",
    "\n",
    "#### 평가\n",
    "- sklearn.metrics # 평가 기법(성능 측정 방법)\n",
    "\n",
    "#### ML 알고리즘들\n",
    "- sklearn.ensemble # 앙상블 알고리즘 (여러 개 묶어서 알고리즘 짜는거)\n",
    "- sklearn.linear_model # 선형 회귀, 릿지, 라쏘, 로지스틱 회귀 등.\n",
    "- sklearn.naive_bayes # 나이브 베이즈 알고리즘, 가우시안 NB, 다항 분포 NB\n",
    "- sklearn.neighbors # 최근접 이웃 알고리즘 (kNN 등)\n",
    "- sklearn.svm # 서포트 벡터 머신\n",
    "- sklearn.tree # 결정트리\n",
    "- sklearn.cluster # 비지도 클러스터링 알고리즘 (Kmeans, 계층형, DBSCAN..)\n",
    "\n",
    "#### 유틸리티 ( 뭐에 써먹는것? 모름)\n",
    "- sklearn.pipeline # 피처 처리 등. 변환과 ML알고리즘 학습, 예측 유틸리티 제공.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 우리는 이 중에서\n",
    "- sklearn.datasets\n",
    "- sklearn.preprocessing\n",
    " - encoder, scaler, standardizer\n",
    "- sklearn.model_selection\n",
    " - train_test_split, GridSearchCV, KFold, Str.KFold, cross_val_score\n",
    "- sklearn.metrics\n",
    " - accuracy_score\n",
    " \n",
    "##### ML 클래스들\n",
    "- sklearn.tree\n",
    "- sklearn.ensemble\n",
    "- sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.datasets import load_iris\n",
    "에 대해서."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. model_selection\n",
    "- train_test_split()\n",
    "- KFold(n_splits=3)\n",
    "- StratifiedKFold(n_splits=3)\n",
    "- GridSearchCV()\n",
    "- cross_val_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도 : 0.9556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.3,\n",
    "                                                    random_state = 121)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "\n",
    "print(\"예측 정확도 :\", np.round(accuracy_score(y_test, pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. KFold(n_splits = 3) : 교차 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5번 교차검증  [1.0, 0.9667, 0.8667, 0.9333, 0.7667]\n",
      "평균 정확도  0.9067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# 피쳐, 레이블 저장\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "\n",
    "# 모델 객체 생성\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# 5개 폴드 세트로 분리해서 교차검증하는 리스트 객체 생성\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_accuracy = []\n",
    "\n",
    "\n",
    "# for문을 이용해서, KFold 내에서 알아서 5개로 나눠준 index들을 가지고 알아서 교차검증...\n",
    "for train_index, test_index in kfold.split(features):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    # 모델 적합 , 예측 , 성능 평가\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "print(\"5번 교차검증 \", cv_accuracy)\n",
    "print(\"평균 정확도 \", np.round(np.mean(cv_accuracy), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. StratifiedKFold(n_splits =3)\n",
    "- Kfold를 위해 나눠지는 n개의 데이터 서브셋의 label 데이터 비율이, 원래 label 데이터의 레이블 비율과 같게 되도록 서브셋을 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3번 str-교차검증  [0.9804, 0.9216, 0.9792]\n",
      "평균 정확도  0.9604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 피쳐, 레이블 데이터 저장\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "\n",
    "# 모델 객체 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "# StKfold 객체 생성\n",
    "skfold = StratifiedKFold(n_splits=3)\n",
    "cv_accuracy = []\n",
    "\n",
    "# for문 돌려서 직접 교차검증하기.. split()호출할때 str.kfold는 무조건 레이블 데이터도 넣어야 함!!\n",
    "for train_index, test_index in skfold.split(features, label):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    # 모델 적합, 예측 및 성능 평가\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    cv_accuracy.append(accuracy)\n",
    "    \n",
    "print(\"3번 str-교차검증 \", cv_accuracy)\n",
    "print(\"평균 정확도 \", np.round(np.mean(cv_accuracy), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일반적으로 분류에서의 교차 검증은 StratifiedKFold로, 회귀는 그냥 KFold로 한다.\n",
    "#### 회귀에서는 레이블의 비율을 측정하는 게 의미가 없기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. cross_val_score() : 교차검증\n",
    "- for문 없이 알아서 해주는 거\n",
    "- 분류의 경우 알아서 StratifiedKFold로 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3번 str-교차검증  [0.9804 0.9216 0.9792]\n",
      "평균 정확도  0.9604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "# from sklearn.metrics import accuracy_score : cross_val_score에서 할꺼니까 필요 없음\n",
    "\n",
    "# 피처, 레이블 데이터\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "\n",
    "# 모델 객체 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state = 156)\n",
    "\n",
    "# cross_val_score() 객체 생성.\n",
    "# 성능지표 : 정확도(accuracy), 교차검증세트 : 3개\n",
    "scores = cross_val_score(dt_clf, features, label, scoring='accuracy', cv=3)\n",
    "print(\"3번 str-교차검증 \", np.round(scores, 4))\n",
    "print(\"평균 정확도 \", np.round(np.mean(scores), 4))\n",
    "\n",
    "# scores 객체는 각 시행에서의 성능 지표를 담은 리스트를 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. GridSearchCV() : 교차 검증과 최적 하이퍼 파라미터 튜닝\n",
    "- 하이퍼 파라미터 : 머신러닝 알고리즘에 매우 중요한 요소 중 하나.\n",
    "- 우리가 직접 설정해야만 함, 그래서 최적의 하이퍼 파라미터를 설정하는 것도 중요하다고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\frank\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 2}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 3}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     params  mean_test_score  rank_test_score  \\\n",
       "0  {'max_depth': 1, 'min_samples_split': 2}         0.700000                5   \n",
       "1  {'max_depth': 1, 'min_samples_split': 3}         0.700000                5   \n",
       "2  {'max_depth': 2, 'min_samples_split': 2}         0.958333                3   \n",
       "3  {'max_depth': 2, 'min_samples_split': 3}         0.958333                3   \n",
       "4  {'max_depth': 3, 'min_samples_split': 2}         0.975000                1   \n",
       "5  {'max_depth': 3, 'min_samples_split': 3}         0.975000                1   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  \n",
       "0              0.700                0.7               0.70  \n",
       "1              0.700                0.7               0.70  \n",
       "2              0.925                1.0               0.95  \n",
       "3              0.925                1.0               0.95  \n",
       "4              0.975                1.0               0.95  \n",
       "5              0.975                1.0               0.95  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score : 필요 없음.\n",
    "\n",
    "\n",
    "# 데이터 로딩, 학습/테스트 분리.\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.2,\n",
    "                                                    random_state = 121)\n",
    "\n",
    "# 모델 객체 생성\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "### 파라미터 조합 생성 : 딕셔너리 형태로. \n",
    "parameters = {'max_depth' : [1, 2, 3], 'min_samples_split' : [2, 3]}\n",
    "\n",
    "# GridSearchCV 객체 생성\n",
    "grid_dtree = GridSearchCV(dtree, param_grid = parameters, cv=3, refit = True)\n",
    "# 3번의 교차검증, 3*2개의 파라미터 조합 => 총 18번 모델 적합 후 최적의 모델 찾아줌\n",
    "\n",
    "# 하이퍼 파라미터 순차적으로 학습평가.\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "# 결과를 데이터프레임 반환\n",
    "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "scores_df[['params', 'mean_test_score', 'rank_test_score',\n",
    "           'split0_test_score', 'split1_test_score', 'split2_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_samples_split': 2}\n",
      "0.975\n",
      "정확도 : 0.9667\n"
     ]
    }
   ],
   "source": [
    "# 최적의 파라미터\n",
    "print(grid_dtree.best_params_)\n",
    "\n",
    "# 최적 파라미터일 때의 정확도\n",
    "print(grid_dtree.best_score_)\n",
    "\n",
    "# 최적 파라미터로 학습된 모델을 반환, refit=True로 해놓아야 함.\n",
    "estimator = grid_dtree.best_estimator_\n",
    "\n",
    "# 예측하기.\n",
    "pred = estimator.predict(X_test)\n",
    "print(\"정확도 : {:.4f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반적으로 __학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행__하고 나서, __별도의 테스트 세트에서 이를 평가__한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. preprocessing\n",
    "- Null 값 : Column 다 버리기에는 너무 중요한 데이터인데, 그렇기엔 Null값이 좀 많은 경우 대체값을 확실하게 잘 찾아야 함. 애매해서 어려움\n",
    "- 문자열 값 : Sklearn ML은 문자열을 입력값으로 허용하지 않는다. -> 인코딩을 통해 숫자형으로 변환되어야 함.\n",
    "- 문자열 피처 : 카테고리형 피처(코드값), 텍스트형 피처(피처 벡터화, 또는 불필요한 피처를 삭제)를 의미\n",
    "- 식별자 문자열 피처와 같은 경우는 인코딩하지 않고 삭제. 굳이 필요가 없음 예측에 중요하지 않으니까\n",
    "------------------------------------------------------------------------------------\n",
    "- 문자열 데이터 인코딩(LabelEncoder(), OneHotEncoder(), pd.get_dummies(df))\n",
    "- 표준화, 정규화(StandardScaler(), MinMaxScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding : LabelEncoder(), OneHotEncoder(), pd.get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환값 : [0 1 4 5 3 3 2 2]\n",
      "인코딩 클래스 : ['TV' '냉장고' '믹서' '선풍기' '전자레인지' '컴퓨터']\n",
      "디코딩 원본값 : ['TV' '냉장고' '전자레인지' '컴퓨터' '선풍기' '선풍기' '믹서' '믹서']\n"
     ]
    }
   ],
   "source": [
    "items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
    "\n",
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# encoder 객체 생성\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# encoder fit\n",
    "encoder.fit(items)\n",
    "\n",
    "# 원하는 데이터에 적용\n",
    "labels = encoder.transform(items)\n",
    "\n",
    "print(\"변환값 :\", labels)\n",
    "print(\"인코딩 클래스 :\", encoder.classes_) # 어떤 카테고리가 어떤 숫자로 치환됐는지\n",
    "print(\"디코딩 원본값 :\", encoder.inverse_transform(labels)) # 숫자를 주면 대응하는 카테고리 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
    "\n",
    "# One-Hot encoding\n",
    "# 주의점 : 모든 변환하기 전 값이 문자열이 아닌 숫자로 되어 있어야 한다.\n",
    "# 주의점 2 : 입력값은 항상 2차원 데이터여야 한다.\n",
    "# 따라서 LabelEncoder를 먼저 적용한 후 적용해야 할 듯\n",
    "\n",
    "# 숫자 값으로 먼저 변환\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "\n",
    "# 2차원 array로 만들어주기\n",
    "labels = labels.reshape((-1, 1))\n",
    "\n",
    "# One-Hot encoding 적용\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "\n",
    "# encode 결과\n",
    "oh_labels.toarray() # oh_labels는 행렬 형태로 나오기 때문에, toarray() 해줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   item_TV  item_냉장고  item_믹서  item_선풍기  item_전자레인지  item_컴퓨터\n",
      "0        1         0        0         0           0         0\n",
      "1        0         1        0         0           0         0\n",
      "2        0         0        0         0           1         0\n",
      "3        0         0        0         0           0         1\n",
      "4        0         0        0         1           0         0\n",
      "5        0         0        0         1           0         0\n",
      "6        0         0        1         0           0         0\n",
      "7        0         0        1         0           0         0\n"
     ]
    }
   ],
   "source": [
    "# pandas의 get_dummies(df) 사용하기\n",
    "\n",
    "item_df = pd.DataFrame({'item' : items})\n",
    "item_dummy_df = pd.get_dummies(item_df)\n",
    "print(item_dummy_df)\n",
    "\n",
    "# 데이터는 대부분 DataFrame 형태이므로 개꿀메소드라고 생각됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정리 : Encoding 과정\n",
    "1. Encoding 하고싶은, 카테고리 데이터가 있는 컬럼을 원본 df에서 빼낸다\n",
    "2. 카테고리 데이터 컬럼을 LabelEncoder에 fit_transform\n",
    "3. 2차원 만든 뒤 OneHotEncoder에 fit_transform\n",
    "4. toarray() 등 사용해서, 원본 df에 다시 추가\n",
    "\n",
    " 또는 빼낸 컬럼을 pd.get_dummies(df)로 원핫인코딩해서 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 피처 스케일링, 정규화\n",
    "\n",
    "- 표준화(Standardization), 정규화(Normalization)\n",
    "- 표준화는 표준정규분포를 따르게 변환하는 것.\n",
    "- 정규화는 최소 0~ 최대 1의 값을 가지도록 min max 변화시켜주는 것.\n",
    "\n",
    "- 사이킷런에서 Normalizer 모듈은 선형대수 정규화 개념으로, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미함.\n",
    "- 표준화 정규화는 한 개의 컬럼에 대해서, 사이킷런의 Normalizer는 동시에 여러 개의 컬럼에 대해서 진행하는 정규화라고 생각할수 있을듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# 피쳐데이터 DataFrame 변환하기.\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)   -1.690315e-15\n",
      "sepal width (cm)    -1.842970e-15\n",
      "petal length (cm)   -1.698641e-15\n",
      "petal width (cm)    -1.409243e-15\n",
      "dtype: float64\n",
      "sepal length (cm)    1.006711\n",
      "sepal width (cm)     1.006711\n",
      "petal length (cm)    1.006711\n",
      "petal width (cm)     1.006711\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler : 표준정규분포 형태로\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler 객체 생성, fit_transform.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(iris_df) # 그냥 df 전체 넣어도 알아서 scale해줌, 개별 피쳐를 변환해준다.\n",
    "iris_scaled = scaler.transform(iris_df) # 반환값은 array형태이다.\n",
    "\n",
    "# array 형태의 scaled 데이터를 다시 DataFrame으로\n",
    "iris_df_scaled = pd.DataFrame(iris_scaled, columns = iris.feature_names)\n",
    "print(iris_df_scaled.mean())\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)    1.0\n",
      "sepal width (cm)     1.0\n",
      "petal length (cm)    1.0\n",
      "petal width (cm)     1.0\n",
      "dtype: float64\n",
      "sepal length (cm)    0.0\n",
      "sepal width (cm)     0.0\n",
      "petal length (cm)    0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler 객체 생성, fit_transform\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# array 형태를 df으로 만들기\n",
    "iris_df_scaled = pd.DataFrame(iris_scaled, columns = iris.feature_names)\n",
    "print(iris_df_scaled.max())\n",
    "print(iris_df_scaled.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터, 테스트 데이터의 스케일링 변환시 유의점\n",
    "\n",
    "- fit(), transform(), fit_transform() 을 이용.\n",
    "- fit() : 객체에 데이터 변환 기준 정보 설정.\n",
    "- transform() : 설정된 정보를 이용해 데이터를 변환\n",
    "- fit_transform() : 한번에 해줌\n",
    "\n",
    "- 그러니까 처음에 학습 데이터 할때는 fit_transform() 해도 되는데, 테스트 데이터를 학습 데이터와 같은 식으로 데이터 변환하려면 fit_transform()이 아니라 transform()만 해야된다!!! fit_transform()하면 테스트 데이터에 맞춘 새로운 스케일링 정보가 저장되어서 그걸로 스케일링되기 때문에 모델이 달라진다.\n",
    "\n",
    "\n",
    "##### 주의사항 정리\n",
    "- __전체 데이터를 스케일링 변환한 뒤 학습-테스트 데이터로 분리할 것__\n",
    "- __그게 안 되면 적어도 fit_transform()을 테스트 데이터에 사용하지 말 것.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
