{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "텐서플로를 이용해 간단한 이미지 분류기를 만들어 볼 생각입니다. by Oreilang\n",
    "'''\n",
    "\n",
    "# 케라스에서 제공하는 데이터셋 다운로드 유틸리티 함수\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "픽셀이 0~255의 값을 가지는 정수이고, 그래서 uint8 (8bit의 unsigned 정수)이다. \n",
    "이걸 255로 나눠서 0~1의 값을 가지도록 변환하고, validation 데이터셋과 train 데이터셋을 분리하겠다.\n",
    "'''\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개의 은닉층으로 이루어진 분류용 다층 퍼셉트론입니다.\n",
    "model = keras.models.Sequential() ## 모델의 layer들의 container. Sequential 모델을 만든다. \n",
    "# 가장 간단한 케라스의 신경망 모델입니다. 순서대로 연결된 층을 일렬로 쌓아서 구성한다. 이걸 Sequential API라고 부릅니다.\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) ## (28, 28) shape의 input을 flatten한다. (X.reshape(-1, 28*28)을 계산하는 것과 동일)\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\")) ## 뉴런 300개 & relu activation을 가지는 Dense 층\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\")) ## 뉴런 100개 & relu activation을 가지는 Dense 층\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) ## 뉴런 10개 & softmax activation을 가지는 Dense층.(출력층은 이런 식으로 작성되어야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(784+1)*300 # 784 : dense_2의 input, 1 : bias. --> input node 수 : (784+1), 300 : 각 input 노드마다 가져야 하는 weight의 수."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x1417064e860>,\n",
       " <keras.layers.core.dense.Dense at 0x141707047f0>,\n",
       " <keras.layers.core.dense.Dense at 0x141707057b0>,\n",
       " <keras.layers.core.dense.Dense at 0x14170705720>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00333101, -0.00870356,  0.02183766, ..., -0.0711413 ,\n",
       "        -0.06405613, -0.00489669],\n",
       "       [ 0.0454184 ,  0.02310432, -0.02287433, ..., -0.01848421,\n",
       "         0.00125761,  0.06927785],\n",
       "       [ 0.03189403,  0.01523967,  0.02961459, ...,  0.00800841,\n",
       "         0.02085385,  0.02280989],\n",
       "       ...,\n",
       "       [-0.02593695, -0.05821452, -0.06201033, ...,  0.03838222,\n",
       "        -0.02246875,  0.06139304],\n",
       "       [-0.0553412 , -0.04364005, -0.01543853, ..., -0.01912199,\n",
       "        -0.05166727, -0.01848609],\n",
       "       [ 0.01699739, -0.02952962, -0.03903154, ...,  0.01390575,\n",
       "        -0.01952998,  0.00988534]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7309 - accuracy: 0.7603 - val_loss: 0.5074 - val_accuracy: 0.8306\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4883 - accuracy: 0.8317 - val_loss: 0.4408 - val_accuracy: 0.8500\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4428 - accuracy: 0.8456 - val_loss: 0.4108 - val_accuracy: 0.8580\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4154 - accuracy: 0.8560 - val_loss: 0.3970 - val_accuracy: 0.8652\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3947 - accuracy: 0.8625 - val_loss: 0.3815 - val_accuracy: 0.8672\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3789 - accuracy: 0.8679 - val_loss: 0.3681 - val_accuracy: 0.8732\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3657 - accuracy: 0.8724 - val_loss: 0.3792 - val_accuracy: 0.8690\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3530 - accuracy: 0.8767 - val_loss: 0.3520 - val_accuracy: 0.8764\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3431 - accuracy: 0.8789 - val_loss: 0.3536 - val_accuracy: 0.8776\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3334 - accuracy: 0.8820 - val_loss: 0.3686 - val_accuracy: 0.8700\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3248 - accuracy: 0.8847 - val_loss: 0.3380 - val_accuracy: 0.8776\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3163 - accuracy: 0.8873 - val_loss: 0.3390 - val_accuracy: 0.8794\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3092 - accuracy: 0.8878 - val_loss: 0.3461 - val_accuracy: 0.8772\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.3011 - accuracy: 0.8930 - val_loss: 0.3286 - val_accuracy: 0.8816\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.2948 - accuracy: 0.8943 - val_loss: 0.3174 - val_accuracy: 0.8856\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2888 - accuracy: 0.8959 - val_loss: 0.3161 - val_accuracy: 0.8872\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2843 - accuracy: 0.8981 - val_loss: 0.3142 - val_accuracy: 0.8880\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2772 - accuracy: 0.9004 - val_loss: 0.3344 - val_accuracy: 0.8834\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2726 - accuracy: 0.9024 - val_loss: 0.3142 - val_accuracy: 0.8878\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2678 - accuracy: 0.9035 - val_loss: 0.3157 - val_accuracy: 0.8880\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2617 - accuracy: 0.9062 - val_loss: 0.3067 - val_accuracy: 0.8922\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2581 - accuracy: 0.9073 - val_loss: 0.2987 - val_accuracy: 0.8896\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2524 - accuracy: 0.9090 - val_loss: 0.3183 - val_accuracy: 0.8874\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2491 - accuracy: 0.9103 - val_loss: 0.3072 - val_accuracy: 0.8940\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2456 - accuracy: 0.9121 - val_loss: 0.3137 - val_accuracy: 0.8844\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2400 - accuracy: 0.9131 - val_loss: 0.2975 - val_accuracy: 0.8944\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2363 - accuracy: 0.9155 - val_loss: 0.2979 - val_accuracy: 0.8930\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2318 - accuracy: 0.9163 - val_loss: 0.2995 - val_accuracy: 0.8972\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2287 - accuracy: 0.9179 - val_loss: 0.2975 - val_accuracy: 0.8922\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2245 - accuracy: 0.9194 - val_loss: 0.2972 - val_accuracy: 0.8950\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32937079668045044, 0.8817999958992004]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test) # loss와 metric을 둘 다 넣었으므로, 두 개의 결과값이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(X_new)\n\u001b[0;32m      2\u001b[0m y_pred\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캘리포니아 주택가격 데이터셋 활용하여 회귀 신경망 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9171 - val_loss: 0.6190\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7376 - val_loss: 0.7256\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9363 - val_loss: 0.4865\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5014 - val_loss: 0.4745\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4637 - val_loss: 0.4462\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4374 - val_loss: 0.4298\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4443 - val_loss: 0.4395\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4268 - val_loss: 0.4261\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4131 - val_loss: 0.4013\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4022 - val_loss: 0.3923\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3938 - val_loss: 0.3867\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3877 - val_loss: 0.3825\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3829 - val_loss: 0.3775\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3796 - val_loss: 0.3794\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3757 - val_loss: 0.3760\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3722 - val_loss: 0.3697\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3669\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3693 - val_loss: 0.3669\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3648 - val_loss: 0.3628\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4010 - val_loss: 0.3795\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3777\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]), # X_train의 data개수가 0, 그 이후가 레코드 하나의 크기.\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=0.01))\n",
    "'''\n",
    "분류 모델일 때는 아래와 같이 compile 했었다.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "회귀 모델일 때의 compile 메소드에는 metrics를 넣지 않는데 이유가 ?\n",
    "'''\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test) # compile 시에 loss만 집어넣었으므로, mse_test도 loss만 나오게 함.\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide & Deep Neural Network를 활용한 캘리포니아 주택 문제 해결\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 구조를 코드로 짜기 전에 먼저 그림으로 구조를 그려놓고, 그에 따라서 해나가는 게 좋을 듯\n",
    "# 애초에 딥러닝 구조를 스스로 짜 놓을 필요성이 앞으로 있을지는 모르겠지만(pretrained model?)\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7949 - val_loss: 0.5446\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4936 - val_loss: 0.5392\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4510 - val_loss: 0.4727\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4274 - val_loss: 0.4353\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4456 - val_loss: 0.4401\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4212 - val_loss: 0.4544\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4016 - val_loss: 0.4122\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3934 - val_loss: 0.4267\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3924 - val_loss: 0.3939\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3811 - val_loss: 0.3859\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3764 - val_loss: 0.3835\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3721 - val_loss: 0.3821\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3666 - val_loss: 0.3794\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3672 - val_loss: 0.3665\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3594 - val_loss: 0.3704\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3554 - val_loss: 0.3575\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3601 - val_loss: 0.3575\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3485 - val_loss: 0.3585\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3468 - val_loss: 0.3595\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3433 - val_loss: 0.3449\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3542\n",
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 서브클래싱 API로 동적 모델 만들기.\n",
    "\n",
    "시퀀셜 API, 함수형 API는 모두 선언적이다. 그러나 어떤 모델의 경우 반복문을 돌거나 하는 등, 커스텀해야 할 것들이 많다. customize를 더 깊은 곳까지 하고 싶다면 서브클래싱 API를 사용해서 직접 층을 구성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)  # 표준 매개변수에 대한 처리\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation, name=\"hidden1\")\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation, name=\"hidden2\")\n",
    "        self.main_output = keras.layers.Dense(1, name=\"main_output\")\n",
    "        self.aux_output = keras.layers.Dense(1, name=\"aux_output\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)     # main_output을 위한 층과 aux_output을 위한 층은 다른 층이므로 다른 변수에 만들어줘야 한다.\n",
    "        return main_output, aux_output\n",
    "\n",
    "model1 = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0151 - output_1_loss: 0.8871 - output_2_loss: 2.1671 - val_loss: 0.6367 - val_output_1_loss: 0.5835 - val_output_2_loss: 1.1160\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5625 - output_1_loss: 0.5154 - output_2_loss: 0.9865 - val_loss: 0.7268 - val_output_1_loss: 0.7056 - val_output_2_loss: 0.9171\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5325 - output_1_loss: 0.5001 - output_2_loss: 0.8240 - val_loss: 0.5051 - val_output_1_loss: 0.4758 - val_output_2_loss: 0.7686\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4862 - output_1_loss: 0.4592 - output_2_loss: 0.7284 - val_loss: 0.4985 - val_output_1_loss: 0.4764 - val_output_2_loss: 0.6975\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4551 - output_1_loss: 0.4321 - output_2_loss: 0.6619 - val_loss: 0.5007 - val_output_1_loss: 0.4758 - val_output_2_loss: 0.7253\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4734 - output_1_loss: 0.4549 - output_2_loss: 0.6400 - val_loss: 0.4858 - val_output_1_loss: 0.4604 - val_output_2_loss: 0.7147\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4400 - output_1_loss: 0.4209 - output_2_loss: 0.6128 - val_loss: 0.4503 - val_output_1_loss: 0.4274 - val_output_2_loss: 0.6562\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4241 - output_1_loss: 0.4056 - output_2_loss: 0.5913 - val_loss: 0.4481 - val_output_1_loss: 0.4325 - val_output_2_loss: 0.5881\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4267 - output_1_loss: 0.4103 - output_2_loss: 0.5736 - val_loss: 0.4468 - val_output_1_loss: 0.4323 - val_output_2_loss: 0.5781\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4150 - output_1_loss: 0.3992 - output_2_loss: 0.5577 - val_loss: 0.4434 - val_output_1_loss: 0.4297 - val_output_2_loss: 0.5666\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4058 - output_1_loss: 0.3908 - output_2_loss: 0.5411 - val_loss: 0.4240 - val_output_1_loss: 0.4099 - val_output_2_loss: 0.5511\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3971 - output_1_loss: 0.3825 - output_2_loss: 0.5278 - val_loss: 0.4223 - val_output_1_loss: 0.4086 - val_output_2_loss: 0.5456\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3961 - output_1_loss: 0.3822 - output_2_loss: 0.5209 - val_loss: 0.4124 - val_output_1_loss: 0.3989 - val_output_2_loss: 0.5337\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3913 - output_1_loss: 0.3775 - output_2_loss: 0.5150 - val_loss: 0.4248 - val_output_1_loss: 0.4127 - val_output_2_loss: 0.5346\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4076 - output_1_loss: 0.3965 - output_2_loss: 0.5077 - val_loss: 0.3878 - val_output_1_loss: 0.3745 - val_output_2_loss: 0.5073\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3802 - output_1_loss: 0.3678 - output_2_loss: 0.4926 - val_loss: 0.3811 - val_output_1_loss: 0.3679 - val_output_2_loss: 0.5006\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3763 - output_1_loss: 0.3640 - output_2_loss: 0.4866 - val_loss: 0.3897 - val_output_1_loss: 0.3777 - val_output_2_loss: 0.4977\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3688 - output_1_loss: 0.3567 - output_2_loss: 0.4778 - val_loss: 0.3824 - val_output_1_loss: 0.3703 - val_output_2_loss: 0.4908\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3670 - output_1_loss: 0.3553 - output_2_loss: 0.4728 - val_loss: 0.3714 - val_output_1_loss: 0.3593 - val_output_2_loss: 0.4802\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3631 - output_1_loss: 0.3516 - output_2_loss: 0.4667 - val_loss: 0.3680 - val_output_1_loss: 0.3566 - val_output_2_loss: 0.4707\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=0.01))\n",
    "history = model1.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                      validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3784 - output_1_loss: 0.3670 - output_2_loss: 0.4806\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model1.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 저장, 복원\n",
    "\n",
    "시퀀셜 API, 함수형 API의 저장은 쉽다고하네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_keras_model1.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\TIL\\ML\\venv_ml\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\TIL\\ML\\venv_ml\\lib\\site-packages\\keras\\saving\\legacy\\save.py:154\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m     save_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mh5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     \u001b[39mor\u001b[39;00m (h5py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, h5py\u001b[39m.\u001b[39mFile))\n\u001b[0;32m    148\u001b[0m     \u001b[39mor\u001b[39;00m saving_utils\u001b[39m.\u001b[39mis_hdf5_filepath(filepath)\n\u001b[0;32m    149\u001b[0m ):\n\u001b[0;32m    150\u001b[0m     \u001b[39m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    152\u001b[0m         model, sequential\u001b[39m.\u001b[39mSequential\n\u001b[0;32m    153\u001b[0m     ):\n\u001b[1;32m--> 154\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    155\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msubclassed models, because such models are defined via the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbody of a Python method, which isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt safely serializable. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConsider saving to the Tensorflow SavedModel format (by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m             \u001b[39m'\u001b[39m\u001b[39msetting save_format=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m) or using `save_weights`.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    161\u001b[0m         )\n\u001b[0;32m    162\u001b[0m     hdf5_format\u001b[39m.\u001b[39msave_model_to_hdf5(\n\u001b[0;32m    163\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model1.save(\"my_keras_model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 콜백 사용하기.\n",
    "\n",
    "`fit()`메서드에서 callbacks 매개변수가 들어갈 수 있고. 이 매개변수는 케라스가 훈련의 시작이나 끝에 호출할 객체 리스트를 지정할 수 있도록 해 준다. (또는 에포크의 시작, 끝, 각 배치 처리 전후에 호출할 수 도 있다. ModelCheckpoint라는 객체는 훈련 동안 일정한 간격으로 체크포인트를 저장함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=(8,))\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.Model(inputs=[input_], outputs=[output])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.0424\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7190\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6696\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6371\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6118\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5915\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5751\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5607\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5489\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5392\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5304\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5244\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5175\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5118\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5065\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5026\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4990\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4944\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4906\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4878\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=20, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=(8,))\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model2 = keras.Model(inputs=[input_], outputs=[output])\n",
    "model2.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.3985 - val_loss: 0.9515\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7892 - val_loss: 0.7021\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7053 - val_loss: 0.6409\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6581 - val_loss: 0.5995\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6230 - val_loss: 0.5692\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5967 - val_loss: 0.5555\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5762 - val_loss: 0.5304\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5605 - val_loss: 0.5174\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5475 - val_loss: 0.5117\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5370 - val_loss: 0.5055\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5282 - val_loss: 0.4916\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5206 - val_loss: 0.4862\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5138 - val_loss: 0.4777\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5084 - val_loss: 0.4739\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5029 - val_loss: 0.4731\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4982 - val_loss: 0.4711\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4942 - val_loss: 0.4639\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4902 - val_loss: 0.4600\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4867 - val_loss: 0.4592\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4830 - val_loss: 0.4679\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb2 = keras.callbacks.ModelCheckpoint(\"my_keras_best_model.h5\", save_best_only=True)\n",
    "history2 = model2.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model(\"my_keras_best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 30)           270         ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 30)           930         ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 38)           0           ['input_7[0][0]',                \n",
      "                                                                  'dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 1)            39          ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x29b2726e620>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.layers[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x29b2726e620>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_layer('dense_34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = best_model.get_layer('dense_34').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EarlyStopping callback\n",
    "\n",
    "`keras.callbacks.EarlyStopping()`을 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4846 - val_loss: 0.4641\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4820 - val_loss: 0.4642\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4799 - val_loss: 0.4586\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4763 - val_loss: 0.4562\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.4740 - val_loss: 0.4571\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4721 - val_loss: 0.4523\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.4496\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.4672 - val_loss: 0.4480\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.4651 - val_loss: 0.4468\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 989us/step - loss: 0.4635 - val_loss: 0.4432\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.4617 - val_loss: 0.4411\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4609 - val_loss: 0.4410\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 979us/step - loss: 0.4586 - val_loss: 0.4386\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.4568 - val_loss: 0.4372\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4552 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4343\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4526 - val_loss: 0.4357\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.4514 - val_loss: 0.4317\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 998us/step - loss: 0.4496 - val_loss: 0.4307\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.4494 - val_loss: 0.4301\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4277\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4461 - val_loss: 0.4275\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4451 - val_loss: 0.4265\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4440 - val_loss: 0.4281\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4239\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4239\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 988us/step - loss: 0.4406 - val_loss: 0.4219\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4398 - val_loss: 0.4238\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4214\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1000us/step - loss: 0.4377 - val_loss: 0.4202\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4195\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4181\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 988us/step - loss: 0.4341 - val_loss: 0.4165\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4166\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4170\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 977us/step - loss: 0.4309 - val_loss: 0.4139\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 977us/step - loss: 0.4301 - val_loss: 0.4134\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 986us/step - loss: 0.4294 - val_loss: 0.4124\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.4289 - val_loss: 0.4129\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.4095\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4263 - val_loss: 0.4100\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.4253 - val_loss: 0.4092\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 998us/step - loss: 0.4245 - val_loss: 0.4083\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.4230 - val_loss: 0.4077\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.4222 - val_loss: 0.4083\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.4212 - val_loss: 0.4068\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.4200 - val_loss: 0.4040\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.4050\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.4037\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.4019\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.4023\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4002\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.3999\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 997us/step - loss: 0.4138 - val_loss: 0.3989\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.3987\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.3987\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.3978\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4109 - val_loss: 0.3955\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.3951\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.4095 - val_loss: 0.3938\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.4090 - val_loss: 0.3946\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.4070 - val_loss: 0.3951\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 971us/step - loss: 0.4056 - val_loss: 0.3928\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 958us/step - loss: 0.4059 - val_loss: 0.3908\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 961us/step - loss: 0.4053 - val_loss: 0.3898\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.3902\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.4027 - val_loss: 0.3896\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.4029 - val_loss: 0.3889\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 973us/step - loss: 0.4011 - val_loss: 0.3871\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.4002 - val_loss: 0.3880\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 949us/step - loss: 0.3992 - val_loss: 0.3858\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 963us/step - loss: 0.3986 - val_loss: 0.3882\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 963us/step - loss: 0.3975 - val_loss: 0.3868\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.3970 - val_loss: 0.3852\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3967 - val_loss: 0.3838\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 973us/step - loss: 0.3965 - val_loss: 0.3823\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3944 - val_loss: 0.3836\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.3822\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.3929 - val_loss: 0.3805\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.3820\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.3798\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3909 - val_loss: 0.3802\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 952us/step - loss: 0.3893 - val_loss: 0.3779\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.3777\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.3886 - val_loss: 0.3774\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3887 - val_loss: 0.3748\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 969us/step - loss: 0.3873 - val_loss: 0.3752\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.3860 - val_loss: 0.3752\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3848 - val_loss: 0.3751\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.3848 - val_loss: 0.3737\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3833 - val_loss: 0.3730\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 998us/step - loss: 0.3826 - val_loss: 0.3743\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.3825 - val_loss: 0.3713\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3812 - val_loss: 0.3712\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 940us/step - loss: 0.3813 - val_loss: 0.3709\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 967us/step - loss: 0.3807 - val_loss: 0.3695\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 970us/step - loss: 0.3797 - val_loss: 0.3696\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 936us/step - loss: 0.3787 - val_loss: 0.3687\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 928us/step - loss: 0.3768 - val_loss: 0.3699\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3772 - val_loss: 0.3668\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)  # 10개 epoch 까지만 참는다.\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 콜백을 만들 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs['val_loss']/logs['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3756\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.3669\n",
      "Epoch 2/100\n",
      "338/363 [==========================>...] - ETA: 0s - loss: 0.3808\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.3653\n",
      "Epoch 3/100\n",
      "337/363 [==========================>...] - ETA: 0s - loss: 0.3824\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.3659\n",
      "Epoch 4/100\n",
      "315/363 [=========================>....] - ETA: 0s - loss: 0.3751\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.3659\n",
      "Epoch 5/100\n",
      "345/363 [===========================>..] - ETA: 0s - loss: 0.3741\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3650\n",
      "Epoch 6/100\n",
      "332/363 [==========================>...] - ETA: 0s - loss: 0.3740\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3636\n",
      "Epoch 7/100\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.3716\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.3716 - val_loss: 0.3646\n",
      "Epoch 8/100\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.3679\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.3627\n",
      "Epoch 9/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3704\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.3704 - val_loss: 0.3623\n",
      "Epoch 10/100\n",
      "331/363 [==========================>...] - ETA: 0s - loss: 0.3721\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.3614\n",
      "Epoch 11/100\n",
      "339/363 [===========================>..] - ETA: 0s - loss: 0.3700\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.3601\n",
      "Epoch 12/100\n",
      "342/363 [===========================>..] - ETA: 0s - loss: 0.3720\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 996us/step - loss: 0.3683 - val_loss: 0.3611\n",
      "Epoch 13/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3684\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 985us/step - loss: 0.3674 - val_loss: 0.3609\n",
      "Epoch 14/100\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3677\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.3675 - val_loss: 0.3621\n",
      "Epoch 15/100\n",
      "341/363 [===========================>..] - ETA: 0s - loss: 0.3648\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.3597\n",
      "Epoch 16/100\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3710\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.3695 - val_loss: 0.3599\n",
      "Epoch 17/100\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.3630\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3611\n",
      "Epoch 18/100\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.3634\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.3634 - val_loss: 0.3581\n",
      "Epoch 19/100\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.3644\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.3643 - val_loss: 0.3582\n",
      "Epoch 20/100\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.3633\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.3630 - val_loss: 0.3571\n",
      "Epoch 21/100\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.3642\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 989us/step - loss: 0.3622 - val_loss: 0.3569\n",
      "Epoch 22/100\n",
      "344/363 [===========================>..] - ETA: 0s - loss: 0.3620\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.3611 - val_loss: 0.3542\n",
      "Epoch 23/100\n",
      "348/363 [===========================>..] - ETA: 0s - loss: 0.3594\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3604 - val_loss: 0.3541\n",
      "Epoch 24/100\n",
      "328/363 [==========================>...] - ETA: 0s - loss: 0.3552\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.3556\n",
      "Epoch 25/100\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3592\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 960us/step - loss: 0.3590 - val_loss: 0.3537\n",
      "Epoch 26/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3601\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3597 - val_loss: 0.3524\n",
      "Epoch 27/100\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.3592\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 969us/step - loss: 0.3580 - val_loss: 0.3536\n",
      "Epoch 28/100\n",
      "295/363 [=======================>......] - ETA: 0s - loss: 0.3485\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 943us/step - loss: 0.3582 - val_loss: 0.3538\n",
      "Epoch 29/100\n",
      "358/363 [============================>.] - ETA: 0s - loss: 0.3593\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 968us/step - loss: 0.3577 - val_loss: 0.3507\n",
      "Epoch 30/100\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.3545\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.3565 - val_loss: 0.3543\n",
      "Epoch 31/100\n",
      "360/363 [============================>.] - ETA: 0s - loss: 0.3553\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3550 - val_loss: 0.3496\n",
      "Epoch 32/100\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.3573\n",
      "val/train: 0.98\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.3566 - val_loss: 0.3510\n",
      "Epoch 33/100\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.3540\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 967us/step - loss: 0.3539 - val_loss: 0.3494\n",
      "Epoch 34/100\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.3618\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 966us/step - loss: 0.3624 - val_loss: 0.3519\n",
      "Epoch 35/100\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3537\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.3579 - val_loss: 0.3530\n",
      "Epoch 36/100\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3516\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.3523 - val_loss: 0.3511\n",
      "Epoch 37/100\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.3538\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.3524 - val_loss: 0.3487\n",
      "Epoch 38/100\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.3494\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3511 - val_loss: 0.3493\n",
      "Epoch 39/100\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3518\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3508 - val_loss: 0.3465\n",
      "Epoch 40/100\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.3514\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3500 - val_loss: 0.3454\n",
      "Epoch 41/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3475\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3497 - val_loss: 0.3476\n",
      "Epoch 42/100\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.3497\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.3496 - val_loss: 0.3453\n",
      "Epoch 43/100\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.3506\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3492 - val_loss: 0.3443\n",
      "Epoch 44/100\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.3480\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 977us/step - loss: 0.3477 - val_loss: 0.3437\n",
      "Epoch 45/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3481\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 993us/step - loss: 0.3487 - val_loss: 0.3503\n",
      "Epoch 46/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3463\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 973us/step - loss: 0.3468 - val_loss: 0.3467\n",
      "Epoch 47/100\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.3467\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.3455 - val_loss: 0.3466\n",
      "Epoch 48/100\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3448\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 979us/step - loss: 0.3468 - val_loss: 0.3431\n",
      "Epoch 49/100\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.3449\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 971us/step - loss: 0.3445 - val_loss: 0.3440\n",
      "Epoch 50/100\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.3453\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.3450 - val_loss: 0.3413\n",
      "Epoch 51/100\n",
      "346/363 [===========================>..] - ETA: 0s - loss: 0.3437\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3426\n",
      "Epoch 52/100\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.3434\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 967us/step - loss: 0.3440 - val_loss: 0.3402\n",
      "Epoch 53/100\n",
      "330/363 [==========================>...] - ETA: 0s - loss: 0.3462\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3428\n",
      "Epoch 54/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3527\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.3504 - val_loss: 0.3394\n",
      "Epoch 55/100\n",
      "345/363 [===========================>..] - ETA: 0s - loss: 0.3458\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.3451 - val_loss: 0.3422\n",
      "Epoch 56/100\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.3404\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.3410 - val_loss: 0.3418\n",
      "Epoch 57/100\n",
      "361/363 [============================>.] - ETA: 0s - loss: 0.3411\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3408 - val_loss: 0.3417\n",
      "Epoch 58/100\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3543\n",
      "val/train: 0.96\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.3532 - val_loss: 0.3387\n",
      "Epoch 59/100\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3506\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 956us/step - loss: 0.3502 - val_loss: 0.3388\n",
      "Epoch 60/100\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.3410\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.3396 - val_loss: 0.3388\n",
      "Epoch 61/100\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.3420\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3366\n",
      "Epoch 62/100\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.3397\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.3399 - val_loss: 0.3370\n",
      "Epoch 63/100\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3382\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 964us/step - loss: 0.3383 - val_loss: 0.3374\n",
      "Epoch 64/100\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.3359\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.3370 - val_loss: 0.3368\n",
      "Epoch 65/100\n",
      "331/363 [==========================>...] - ETA: 0s - loss: 0.3368\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3364\n",
      "Epoch 66/100\n",
      "348/363 [===========================>..] - ETA: 0s - loss: 0.3367\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.3383\n",
      "Epoch 67/100\n",
      "348/363 [===========================>..] - ETA: 0s - loss: 0.3543\n",
      "val/train: 0.95\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.3340\n",
      "Epoch 68/100\n",
      "345/363 [===========================>..] - ETA: 0s - loss: 0.3384\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 990us/step - loss: 0.3395 - val_loss: 0.3384\n",
      "Epoch 69/100\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.3375\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.3366 - val_loss: 0.3343\n",
      "Epoch 70/100\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.3341\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3342 - val_loss: 0.3376\n",
      "Epoch 71/100\n",
      "298/363 [=======================>......] - ETA: 0s - loss: 0.3358\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3369\n",
      "Epoch 72/100\n",
      "360/363 [============================>.] - ETA: 0s - loss: 0.3329\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 982us/step - loss: 0.3338 - val_loss: 0.3341\n",
      "Epoch 73/100\n",
      "319/363 [=========================>....] - ETA: 0s - loss: 0.3440\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3351 - val_loss: 0.3333\n",
      "Epoch 74/100\n",
      "342/363 [===========================>..] - ETA: 0s - loss: 0.3355\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3347\n",
      "Epoch 75/100\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3330\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3329 - val_loss: 0.3320\n",
      "Epoch 76/100\n",
      "331/363 [==========================>...] - ETA: 0s - loss: 0.3278\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.3335\n",
      "Epoch 77/100\n",
      "336/363 [==========================>...] - ETA: 0s - loss: 0.3338\n",
      "val/train: 0.99\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.3308\n",
      "Epoch 78/100\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 0.3345\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3325 - val_loss: 0.3322\n",
      "Epoch 79/100\n",
      "324/363 [=========================>....] - ETA: 0s - loss: 0.3341\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.3321\n",
      "Epoch 80/100\n",
      "335/363 [==========================>...] - ETA: 0s - loss: 0.3282\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3312\n",
      "Epoch 81/100\n",
      "339/363 [===========================>..] - ETA: 0s - loss: 0.3314\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3324\n",
      "Epoch 82/100\n",
      "340/363 [===========================>..] - ETA: 0s - loss: 0.3318\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 994us/step - loss: 0.3336 - val_loss: 0.3336\n",
      "Epoch 83/100\n",
      "346/363 [===========================>..] - ETA: 0s - loss: 0.3293\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3288 - val_loss: 0.3311\n",
      "Epoch 84/100\n",
      "320/363 [=========================>....] - ETA: 0s - loss: 0.3280\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3296 - val_loss: 0.3300\n",
      "Epoch 85/100\n",
      "337/363 [==========================>...] - ETA: 0s - loss: 0.3318\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3288\n",
      "Epoch 86/100\n",
      "316/363 [=========================>....] - ETA: 0s - loss: 0.3291\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3282 - val_loss: 0.3323\n",
      "Epoch 87/100\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.3288\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.3315\n",
      "Epoch 88/100\n",
      "362/363 [============================>.] - ETA: 0s - loss: 0.3282\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3280\n",
      "Epoch 89/100\n",
      "333/363 [==========================>...] - ETA: 0s - loss: 0.3283\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.3273\n",
      "Epoch 90/100\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.3227\n",
      "val/train: 1.03\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.3345\n",
      "Epoch 91/100\n",
      "330/363 [==========================>...] - ETA: 0s - loss: 0.3247\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3272 - val_loss: 0.3264\n",
      "Epoch 92/100\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.3248\n",
      "val/train: 1.02\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3264 - val_loss: 0.3315\n",
      "Epoch 93/100\n",
      "340/363 [===========================>..] - ETA: 0s - loss: 0.3233\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3251 - val_loss: 0.3292\n",
      "Epoch 94/100\n",
      "322/363 [=========================>....] - ETA: 0s - loss: 0.3257\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.3273\n",
      "Epoch 95/100\n",
      "327/363 [==========================>...] - ETA: 0s - loss: 0.3294\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3275\n",
      "Epoch 96/100\n",
      "332/363 [==========================>...] - ETA: 0s - loss: 0.3296\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3251 - val_loss: 0.3285\n",
      "Epoch 97/100\n",
      "342/363 [===========================>..] - ETA: 0s - loss: 0.3539\n",
      "val/train: 0.92\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.3248\n",
      "Epoch 98/100\n",
      "320/363 [=========================>....] - ETA: 0s - loss: 0.3247\n",
      "val/train: 1.00\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3257 - val_loss: 0.3267\n",
      "Epoch 99/100\n",
      "328/363 [==========================>...] - ETA: 0s - loss: 0.3272\n",
      "val/train: 1.01\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3279\n",
      "Epoch 100/100\n",
      "335/363 [==========================>...] - ETA: 0s - loss: 0.3219\n",
      "val/train: 1.02\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3291\n"
     ]
    }
   ],
   "source": [
    "my_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb, my_cb]\n",
    ")\n",
    "\n",
    "# 왜 에포크마다 두 줄씩 나와?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorboard 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3281\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.3261\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3220 - val_loss: 0.3242\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.3235\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3224\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3270\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3215 - val_loss: 0.3225\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3208 - val_loss: 0.3248\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3239\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.3217\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3198 - val_loss: 0.3236\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3190 - val_loss: 0.3233\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 975us/step - loss: 0.3190 - val_loss: 0.3226\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3183 - val_loss: 0.3234\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.3240\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.3215\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3178 - val_loss: 0.3200\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3195\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3175 - val_loss: 0.3204\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.3210\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3204\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3222\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3203\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3153 - val_loss: 0.3192\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 0.3179\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3151 - val_loss: 0.3192\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3149 - val_loss: 0.3196\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 995us/step - loss: 0.3144 - val_loss: 0.3170\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.3178\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.3195\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=30,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[tensorboard_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv_ml': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34f79edca71b491da1184107855aae9e808535a0ee8a0ea99ef7c0ae573f9e6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
