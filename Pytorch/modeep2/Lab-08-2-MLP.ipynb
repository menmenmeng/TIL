{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR을 위해 MLP가 필요하다. 여러 개 층을 갖는 MultiLayer Perceptron.\n",
    "# MLP를 학습할 방법이 없다. 고 생각했는데, Backprop을 통해 해결함.\n",
    "# Output - GT = Loss. 이 loss값을 최소화할 수 있도록 weight를 업데이트하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7949647903442383\n",
      "100 0.6927714347839355\n",
      "200 0.6920170783996582\n",
      "300 0.6874877214431763\n",
      "400 0.6438971757888794\n",
      "500 0.5434775948524475\n",
      "600 0.31091856956481934\n",
      "700 0.09974604845046997\n",
      "800 0.05250263586640358\n",
      "900 0.034888073801994324\n",
      "1000 0.02593059279024601\n",
      "1100 0.02055979147553444\n",
      "1200 0.016997918486595154\n",
      "1300 0.014469827525317669\n",
      "1400 0.012585810385644436\n",
      "1500 0.011129355058073997\n",
      "1600 0.0099706519395113\n",
      "1700 0.009027538821101189\n",
      "1800 0.008245264180004597\n",
      "1900 0.007586242631077766\n",
      "2000 0.0070235878229141235\n",
      "2100 0.006537777371704578\n",
      "2200 0.006114100571721792\n",
      "2300 0.005741457920521498\n",
      "2400 0.00541120208799839\n",
      "2500 0.005116503220051527\n",
      "2600 0.004851953126490116\n",
      "2700 0.004613138735294342\n",
      "2800 0.004396582487970591\n",
      "2900 0.004199261777102947\n",
      "3000 0.004018725827336311\n",
      "3100 0.003852915484458208\n",
      "3200 0.0037001771852374077\n",
      "3300 0.0035589945036917925\n",
      "3400 0.003428105963394046\n",
      "3500 0.0033064461313188076\n",
      "3600 0.003193024080246687\n",
      "3700 0.0030870898626744747\n",
      "3800 0.0029879387002438307\n",
      "3900 0.0028948509134352207\n",
      "4000 0.0028073610737919807\n",
      "4100 0.0027249897830188274\n",
      "4200 0.002647272776812315\n",
      "4300 0.0025738351978361607\n",
      "4400 0.002504347125068307\n",
      "4500 0.002438479335978627\n",
      "4600 0.002375992015004158\n",
      "4700 0.002316526137292385\n",
      "4800 0.00226005120202899\n",
      "4900 0.0022061928175389767\n",
      "5000 0.002154846675693989\n",
      "5100 0.0021057731937617064\n",
      "5200 0.0020589125342667103\n",
      "5300 0.0020140549167990685\n",
      "5400 0.0019711253698915243\n",
      "5500 0.0019299746491014957\n",
      "5600 0.0018904979806393385\n",
      "5700 0.0018526052590459585\n",
      "5800 0.001816162490285933\n",
      "5900 0.0017811240395531058\n",
      "6000 0.001747445436194539\n",
      "6100 0.001714962418191135\n",
      "6200 0.0016837044386193156\n",
      "6300 0.0016535221366211772\n",
      "6400 0.0016243855934590101\n",
      "6500 0.0015962799079716206\n",
      "6600 0.0015691003063693643\n",
      "6700 0.0015428767073899508\n",
      "6800 0.0015174747677519917\n",
      "6900 0.001492864335887134\n",
      "7000 0.0014690605457872152\n",
      "7100 0.0014460034435614944\n",
      "7200 0.0014236632268875837\n",
      "7300 0.0014020099770277739\n",
      "7400 0.0013809988740831614\n",
      "7500 0.0013605852145701647\n",
      "7600 0.0013407388469204307\n",
      "7700 0.0013214896898716688\n",
      "7800 0.001302808173932135\n",
      "7900 0.0012846043100580573\n",
      "8000 0.0012669082498177886\n",
      "8100 0.0012496898416429758\n",
      "8200 0.0012329940218478441\n",
      "8300 0.0012166565284132957\n",
      "8400 0.0012007817858830094\n",
      "8500 0.0011852951720356941\n",
      "8600 0.0011702266056090593\n",
      "8700 0.001155501464381814\n",
      "8800 0.0011411495506763458\n",
      "8900 0.0011271558469161391\n",
      "9000 0.0011135352542623878\n",
      "9100 0.0011001834645867348\n",
      "9200 0.00108714506495744\n",
      "9300 0.0010744647588580847\n",
      "9400 0.0010620086686685681\n",
      "9500 0.0010499105555936694\n",
      "9600 0.0010380512103438377\n",
      "9700 0.0010264308657497168\n",
      "9800 0.0010150790913030505\n",
      "9900 0.0010039809858426452\n",
      "10000 0.0009931366657838225\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([0, 1, 1, 0]).view(4, 1)\n",
    "\n",
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "for step in range(10001):\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if step%100==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4842e-04],\n",
       "        [9.9912e-01],\n",
       "        [9.9869e-01],\n",
       "        [8.2923e-04]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 게이트가 잘 나오는거 확인가능.\n",
    "# torch에서는 layer, activation function, evaluation function이 모두 torch.nn에 있나 봄\n",
    "# optimizer는 torch.optim에 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.69317626953125\n",
      "100 0.6931334733963013\n",
      "200 0.6931323409080505\n",
      "300 0.6931310296058655\n",
      "400 0.6931297183036804\n",
      "500 0.6931282877922058\n",
      "600 0.6931267976760864\n",
      "700 0.6931250095367432\n",
      "800 0.6931232810020447\n",
      "900 0.6931213140487671\n",
      "1000 0.6931192874908447\n",
      "1100 0.6931169629096985\n",
      "1200 0.6931145191192627\n",
      "1300 0.6931117177009583\n",
      "1400 0.6931086778640747\n",
      "1500 0.6931052207946777\n",
      "1600 0.6931014060974121\n",
      "1700 0.6930971145629883\n",
      "1800 0.6930922865867615\n",
      "1900 0.6930868029594421\n",
      "2000 0.6930803060531616\n",
      "2100 0.6930729746818542\n",
      "2200 0.6930643320083618\n",
      "2300 0.69305419921875\n",
      "2400 0.693041980266571\n",
      "2500 0.6930273771286011\n",
      "2600 0.6930093765258789\n",
      "2700 0.6929869651794434\n",
      "2800 0.6929585933685303\n",
      "2900 0.6929216980934143\n",
      "3000 0.6928725242614746\n",
      "3100 0.6928048729896545\n",
      "3200 0.6927081942558289\n",
      "3300 0.6925627589225769\n",
      "3400 0.6923301219940186\n",
      "3500 0.6919246912002563\n",
      "3600 0.6911283135414124\n",
      "3700 0.689253568649292\n",
      "3800 0.6832619905471802\n",
      "3900 0.6502701640129089\n",
      "4000 0.4553481340408325\n",
      "4100 0.035030800849199295\n",
      "4200 0.00815008208155632\n",
      "4300 0.004183229524642229\n",
      "4400 0.0027289779391139746\n",
      "4500 0.0019962082151323557\n",
      "4600 0.0015609061811119318\n",
      "4700 0.001274914713576436\n",
      "4800 0.0010737397242337465\n",
      "4900 0.0009250640869140625\n",
      "5000 0.0008110261987894773\n",
      "5100 0.0007209510076791048\n",
      "5200 0.000648151442874223\n",
      "5300 0.000588120601605624\n",
      "5400 0.0005378293571993709\n",
      "5500 0.0004951889859512448\n",
      "5600 0.0004585586430039257\n",
      "5700 0.0004267299664206803\n",
      "5800 0.0003988826065324247\n",
      "5900 0.00037436030106619\n",
      "6000 0.0003525218344293535\n",
      "6100 0.0003330092295072973\n",
      "6200 0.00031544972443953156\n",
      "6300 0.0002996196271851659\n",
      "6400 0.00028520580963231623\n",
      "6500 0.0002721038763411343\n",
      "6600 0.00026009022258222103\n",
      "6700 0.0002490455226507038\n",
      "6800 0.0002388803695794195\n",
      "6900 0.0002294605365023017\n",
      "7000 0.00022074129083193839\n",
      "7100 0.0002126481558661908\n",
      "7200 0.0002051065384875983\n",
      "7300 0.00019805679039563984\n",
      "7400 0.0001914691529236734\n",
      "7500 0.00018525414634495974\n",
      "7600 0.00017945650324691087\n",
      "7700 0.00017397187184542418\n",
      "7800 0.00016885985678527504\n",
      "7900 0.0001639863330638036\n",
      "8000 0.0001593661872902885\n",
      "8100 0.00015501434972975403\n",
      "8200 0.0001508711720816791\n",
      "8300 0.0001469366397941485\n",
      "8400 0.0001432107965229079\n",
      "8500 0.00013963399396743625\n",
      "8600 0.00013623603445012122\n",
      "8700 0.00013303184823598713\n",
      "8800 0.00012988726666662842\n",
      "8900 0.00012695134500972927\n",
      "9000 0.0001241197605850175\n",
      "9100 0.00012142230843892321\n",
      "9200 0.00011882917897310108\n",
      "9300 0.00011632547830231488\n",
      "9400 0.00011394101602490991\n",
      "9500 0.00011164596071466804\n",
      "9600 0.00010942544031422585\n",
      "9700 0.00010727942571975291\n",
      "9800 0.0001052824518410489\n",
      "9900 0.00010328547796234488\n",
      "10000 0.00010137792560271919\n"
     ]
    }
   ],
   "source": [
    "# XOR - nn-wide-deep\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = torch.FloatTensor([0, 1, 1, 0]).view(4, 1)\n",
    "\n",
    "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
    "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for step in range(10001):\n",
    "    hypothesis = model(X)\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if step%100==0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f8b5fff75b2d8ca621f0b371f1d64751a06b7789195e8887ef7855e4487b82"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
