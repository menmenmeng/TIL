{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "W = torch.zeros(3, requires_grad=True) # 1*3 matrix지만, 5*3 @ 1*3 할 때는 자동으로 5*3 @ 3*1 로 간주하나 봄\n",
    "# W = torch.zeros((3, 1), requires_grad=True) # 이걸로 할 수도 있음!\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "hypothesis = x_train @ W + b\n",
    "cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "# optimizer 정의\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "# optimizer 사용 및 step\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:1 | W:tensor([0., 0., 0.], grad_fn=<SqueezeBackward0>) | b:0.000\n",
      "ep:2 | W:tensor([0.2940, 0.2936, 0.2974], grad_fn=<SqueezeBackward0>) | b:0.003\n",
      "ep:3 | W:tensor([0.4586, 0.4579, 0.4639], grad_fn=<SqueezeBackward0>) | b:0.005\n",
      "ep:4 | W:tensor([0.5508, 0.5499, 0.5571], grad_fn=<SqueezeBackward0>) | b:0.006\n",
      "ep:5 | W:tensor([0.6025, 0.6014, 0.6093], grad_fn=<SqueezeBackward0>) | b:0.007\n",
      "ep:6 | W:tensor([0.6314, 0.6302, 0.6385], grad_fn=<SqueezeBackward0>) | b:0.007\n",
      "ep:7 | W:tensor([0.6476, 0.6463, 0.6549], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:8 | W:tensor([0.6568, 0.6553, 0.6640], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:9 | W:tensor([0.6619, 0.6603, 0.6692], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:10 | W:tensor([0.6648, 0.6630, 0.6720], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:11 | W:tensor([0.6664, 0.6646, 0.6737], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:12 | W:tensor([0.6674, 0.6654, 0.6746], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:13 | W:tensor([0.6679, 0.6658, 0.6751], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:14 | W:tensor([0.6683, 0.6660, 0.6754], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:15 | W:tensor([0.6685, 0.6661, 0.6755], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:16 | W:tensor([0.6686, 0.6662, 0.6756], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:17 | W:tensor([0.6687, 0.6661, 0.6757], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:18 | W:tensor([0.6688, 0.6661, 0.6757], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:19 | W:tensor([0.6689, 0.6661, 0.6757], grad_fn=<SqueezeBackward0>) | b:0.008\n",
      "ep:20 | W:tensor([0.6689, 0.6660, 0.6757], grad_fn=<SqueezeBackward0>) | b:0.008\n"
     ]
    }
   ],
   "source": [
    "# Full code with epochs\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    hypothesis = x_train @ W + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "    print(f\"ep:{epoch} | W:{W.squeeze()} | b:{b.item():.3f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep:1 | pred:tensor([29.9918, 37.8088, 36.2477, 40.2650, 28.6965]) | cost:18861.404297\n",
      "ep:2 | pred:tensor([ 83.6247, 102.2720,  99.7642, 109.4329,  77.8659]) | cost:5912.791016\n",
      "ep:3 | pred:tensor([113.6517, 138.3626, 135.3247, 148.1573, 105.3941]) | cost:1854.087158\n",
      "ep:4 | pred:tensor([130.4626, 158.5685, 155.2337, 169.8377, 120.8062]) | cost:581.899292\n",
      "ep:5 | pred:tensor([139.8743, 169.8811, 166.3800, 181.9757, 129.4350]) | cost:183.135651\n",
      "ep:6 | pred:tensor([145.1434, 176.2147, 172.6203, 188.7713, 134.2661]) | cost:58.144054\n",
      "ep:7 | pred:tensor([148.0934, 179.7608, 176.1141, 192.5759, 136.9709]) | cost:18.965687\n",
      "ep:8 | pred:tensor([149.7448, 181.7461, 178.0701, 194.7059, 138.4853]) | cost:6.685191\n",
      "ep:9 | pred:tensor([150.6693, 182.8577, 179.1651, 195.8984, 139.3333]) | cost:2.835761\n",
      "ep:10 | pred:tensor([151.1867, 183.4801, 179.7782, 196.5660, 139.8082]) | cost:1.628994\n",
      "ep:11 | pred:tensor([151.4763, 183.8287, 180.1213, 196.9397, 140.0742]) | cost:1.250555\n",
      "ep:12 | pred:tensor([151.6384, 184.0239, 180.3134, 197.1489, 140.2232]) | cost:1.131759\n",
      "ep:13 | pred:tensor([151.7290, 184.1332, 180.4210, 197.2660, 140.3067]) | cost:1.094359\n",
      "ep:14 | pred:tensor([151.7796, 184.1945, 180.4811, 197.3315, 140.3536]) | cost:1.082458\n",
      "ep:15 | pred:tensor([151.8079, 184.2289, 180.5148, 197.3681, 140.3799]) | cost:1.078565\n",
      "ep:16 | pred:tensor([151.8236, 184.2483, 180.5336, 197.3886, 140.3948]) | cost:1.077162\n",
      "ep:17 | pred:tensor([151.8323, 184.2592, 180.5441, 197.4000, 140.4032]) | cost:1.076556\n",
      "ep:18 | pred:tensor([151.8370, 184.2653, 180.5500, 197.4064, 140.4081]) | cost:1.076185\n",
      "ep:19 | pred:tensor([151.8396, 184.2689, 180.5532, 197.4099, 140.4109]) | cost:1.075902\n",
      "ep:20 | pred:tensor([151.8409, 184.2709, 180.5550, 197.4118, 140.4126]) | cost:1.075635\n"
     ]
    }
   ],
   "source": [
    "# W, b를 매번 정의해서 사용하면 비효율적, nn.module이라는 편리한 모듈을 제공하고 있다.\n",
    "# Full code with nn.module\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = MultivariateLinearRegressionModel() # 직접 module을 만들어야 해?\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "nb_epochs = 20\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "\n",
    "    pred = model(x_train)\n",
    "    cost = F.mse_loss(pred, y_train) # mse loss function 사용\n",
    "\n",
    "    print(f\"ep:{epoch} | pred:{pred.squeeze().detach()} | cost:{cost:.6f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89f8b5fff75b2d8ca621f0b371f1d64751a06b7789195e8887ef7855e4487b82"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
